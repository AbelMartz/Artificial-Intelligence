{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction\n",
                "The Vacuum Cleaner World problem simulates an intelligent agent tasked with cleaning a structured environment. This project implements and evaluates three distinct agent types, each employing different decision-making strategies:\n",
                "\n",
                "**Simple Reflex Agent:** Acts solely based on the current percept, without memory or long-term planning.\n",
                "\n",
                "**Randomized Reflex Agent:** Makes random decisions, selecting from movement and cleaning actions unpredictably.\n",
                "\n",
                "**State-Aware Reflex Agent:** Remembers visited locations and prioritizes unvisited or dirty spots, improving efficiency.\n",
                "\n",
                "The primary objective of this study is to analyze and compare the performance of these agents, focusing on their efficiency, adaptability, and decision-making effectiveness across different environments.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Work Distribution\n",
                "The project tasks were divided as follows:\n",
                "\n",
                "**Diego Green & Jason Gutierrez:** Implemented the Simple Reflex Agent, Randomized Reflex Agent, and State-Aware Reflex Agent, along with their corresponding tests.\n",
                "\n",
                "**Abel Martinez:** Wrote the hypothesis, conclusion, and presentation.\n",
                "\n",
                "**Nelson Nin:** Wrote the analysis, purpose, and presentation.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Purpose\n",
                "This assignment aims to evaluate the efficiency of different vacuum cleaner agents in cleaning a structured environment under varying conditions. By analyzing their actions and performance scores, we assess the impact of state awareness, randomness, and simple reflex-based decision-making on overall effectiveness."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hypothesis\n",
                "We hypothesize that an agent with greater environmental awareness—such as the State-Aware Reflex Agent—will achieve the highest performance. This is because it can track visited locations and prioritize uncleaned areas, avoiding unnecessary movements. In contrast, the Randomized Reflex Agent is expected to perform the worst due to its unpredictable actions, which may lead to inefficient cleaning paths. The Simple Reflex Agent is anticipated to perform better than the randomized agent but may still face inefficiencies, such as getting stuck in loops when no immediate dirt is detected."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Concepts\n",
                "\n",
                "This experiment is based on the following key concepts:\n",
                "\n",
                "- **Reflex agents**: Agents that make decisions purely based on their current perceptions, without considering past actions or experiences.\n",
                "- **State-Aware agents**: Agents that keep track of the environment’s state, using previous observations to make more informed decisions.\n",
                "- **Performance evaluation**: Measuring an agent's effectiveness by assessing how efficiently it cleans the environment while minimizing unnecessary movements.\n",
                "- **Simulated environment**: A controlled setting designed to test and compare the behaviors of different agents under predefined conditions.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experimental Design\n",
                "\n",
                "This experiment involves the implementation of three distinct agent types: Simple Reflex, Randomized Reflex, and State-Aware Reflex. Each agent is programmed to clean dirty locations and navigate between different positions within the environment. Their performance is evaluated based on the number of locations cleaned and the efficiency of their movements. \n",
                "\n",
                "To assess their effectiveness, we track their actions and analyze their decision-making processes. The experiment takes place in a structured environment consisting of two locations, where the dirt status can be adjusted to test the agents under different conditions.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Code\n",
                "\n",
                "This section presents the implementation of the environment and the agents used in this experiment. It includes the **Simple Reflex Agent**, **Randomized Reflex Agent**, and **State-Aware Reflex Agent**, each designed with distinct decision-making approaches. Additionally, this section covers the environment setup and the logic for measuring agent performance, ensuring a structured evaluation of their efficiency. A mix of our code and the repository of UC Berkley was utilized to conduct the experiments [2]. \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exercise 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initial state of the environment: {(0, 0): 'Clean', (1, 0): 'Dirty'}\n",
                        "Current environment state: {(0, 0): 'Clean', (1, 0): 'Clean'}\n",
                        "Agent is at location (1, 0).\n",
                        "Current environment state: {(0, 0): 'Clean', (1, 0): 'Clean'}\n",
                        "Agent is at location (0, 0).\n",
                        "Current environment state: {(0, 0): 'Clean', (1, 0): 'Clean'}\n",
                        "Agent is at location (1, 0).\n",
                        "Total performance score: 8\n"
                    ]
                }
            ],
            "source": [
                "from agents import Agent, Environment, Thing\n",
                "import random\n",
                "\n",
                "# Global variables\n",
                "visited_locations = set()\n",
                "performance_score = 0\n",
                "\n",
                "# Define grid locations where agent will move and clean\n",
                "locations = [(0, 0), (1, 0)]  # Simple 2-location environment\n",
                "\n",
                "# Action handling based on agent's decisions\n",
                "def action_handler(environment, agent, action):\n",
                "    global performance_score\n",
                "    if action == 'MoveRight':\n",
                "        if agent.location == locations[-1]:\n",
                "            agent.location = locations[-1]\n",
                "            agent.performance -= 1\n",
                "            performance_score = agent.performance\n",
                "        else:\n",
                "            agent.location = locations[locations.index(agent.location) + 1]\n",
                "            agent.performance -= 1\n",
                "            performance_score = agent.performance\n",
                "    elif action == 'MoveLeft':\n",
                "        if agent.location == locations[0]:\n",
                "            agent.location = locations[0]\n",
                "            agent.performance -= 1\n",
                "            performance_score = agent.performance\n",
                "        else:\n",
                "            agent.location = locations[locations.index(agent.location) - 1]\n",
                "            agent.performance -= 1\n",
                "            performance_score = agent.performance\n",
                "    elif action == 'Clean':\n",
                "        if environment.status[agent.location] == 'Dirty':\n",
                "            agent.performance += 10\n",
                "            performance_score = agent.performance\n",
                "        environment.status[agent.location] = 'Clean'\n",
                "\n",
                "# Set up the environment state (Dirty or Clean)\n",
                "def setup_environment(environment):\n",
                "    state = {}\n",
                "    for loc in locations:\n",
                "        user_input = input(f\"Set location {loc} to be clean? (Y/N): \").upper()\n",
                "        if user_input == 'Y':\n",
                "            state[loc] = 'Clean'\n",
                "        elif user_input == 'N':\n",
                "            state[loc] = 'Dirty'\n",
                "        else:\n",
                "            state[loc] = random.choice(['Clean', 'Dirty'])\n",
                "            print(\"Random state selected for location.\")\n",
                "    environment.status = state\n",
                "\n",
                "# Select agent's starting position\n",
                "def select_starting_position(environment):\n",
                "    entry = input(f\"Choose starting position (0 to {len(locations) - 1}): \")\n",
                "    if entry == '':\n",
                "        print(\"Random starting position selected.\")\n",
                "        return random.choice(locations)\n",
                "    elif int(entry) < len(locations) and int(entry) >= 0:\n",
                "        return locations[int(entry)]\n",
                "    else:\n",
                "        print(\"Invalid entry. Random starting position selected.\")\n",
                "        return random.choice(locations)\n",
                "\n",
                "# Define agent's program to decide actions\n",
                "def agent_decision_program():\n",
                "    def program(percept):\n",
                "        location, status = percept\n",
                "        visited_locations.add(location)\n",
                "\n",
                "        if status == 'Dirty':\n",
                "            return 'Clean'  # Clean if dirty\n",
                "\n",
                "        # If the agent is at location 0, move to the right\n",
                "        if locations.index(location) == 0:\n",
                "            return 'MoveRight'\n",
                "        # If the agent is at the last location, move to the left\n",
                "        elif locations.index(location) == len(locations) - 1:\n",
                "            return 'MoveLeft'\n",
                "        return 'MoveRight'\n",
                "    return program\n",
                "\n",
                "# Define the environment class to handle the agent's world\n",
                "class CustomEnvironment(Environment):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        setup_environment(self)\n",
                "\n",
                "    def percept(self, agent):\n",
                "        \"\"\"Returns the agent's location and its status (Dirty or Clean).\"\"\"\n",
                "        return agent.location, self.status[agent.location]\n",
                "\n",
                "    def execute_action(self, agent, action):\n",
                "        \"\"\"Executes the agent's action.\"\"\"\n",
                "        action_handler(self, agent, action)\n",
                "\n",
                "    def thing_classes(self):\n",
                "        \"\"\"Classes of things in the environment.\"\"\"\n",
                "        return [Thing]\n",
                "\n",
                "    def default_location(self, thing):\n",
                "        \"\"\"Set agent's starting position.\"\"\"\n",
                "        return select_starting_position(self)\n",
                "\n",
                "# Running the simulation\n",
                "def run_simulation(agent_program, environment):\n",
                "    global performance_score\n",
                "\n",
                "    # Create the agent\n",
                "    agent = Agent(program=agent_program)\n",
                "\n",
                "    # Set agent's starting position\n",
                "    agent.location = select_starting_position(environment)\n",
                "\n",
                "    # Run until all locations have been visited\n",
                "    while len(visited_locations) < len(locations):\n",
                "        # Use the program method of the agent to get its next action\n",
                "        environment.execute_action(agent, agent.program(environment.percept(agent)))\n",
                "        print(f\"Current environment state: {environment.status}\")\n",
                "        print(f\"Agent is at location {agent.location}.\")\n",
                "\n",
                "    print(f\"Total performance score: {performance_score}\")\n",
                "\n",
                "# Setup and start the environment with the agent\n",
                "environment = CustomEnvironment()\n",
                "print(\"Initial state of the environment:\", environment.status)\n",
                "\n",
                "# Start the simulation\n",
                "run_simulation(agent_decision_program(), environment)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exercise 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running Reflex Agent\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 0\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -1\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -1\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -2\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -3\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -4\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -4\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -5\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -6\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -6\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -6\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -7\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -7\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -7\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -7\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -7\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -8\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -9\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -9\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: -9\n",
                        "Final Score: -9\n",
                        "\n",
                        "Agent at (1, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Dirty', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Dirty'}, Score: 10\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Dirty', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Dirty'}, Score: 9\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Dirty', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 19\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Dirty', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 19\n",
                        "Agent at (1, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Dirty', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 18\n",
                        "Agent at (1, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Dirty', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 18\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Dirty', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 17\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 27\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 27\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 27\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 26\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 25\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 24\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 23\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 22\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 21\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 21\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 21\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 21\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 21\n",
                        "Final Score: 21\n",
                        "\n",
                        "Agent at (0, 3), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 10\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 9\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 19\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 19\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 19\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 18\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 17\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 27\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 27\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 27\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 27\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 26\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 25\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 24\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 34\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 34\n",
                        "Agent at (2, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 33\n",
                        "Agent at (2, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 43\n",
                        "Agent at (3, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 42\n",
                        "Agent at (3, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Dirty', (3, 2): 'Dirty', (3, 3): 'Clean'}, Score: 52\n",
                        "Final Score: 52\n",
                        "\n",
                        "Running Random Agent\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty'}, Score: 0\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 10\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 10\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 10\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 10\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 10\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 9\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 9\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 8\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 8\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 8\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 8\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 8\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 8\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 7\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 6\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 5\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 5\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 5\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean'}, Score: 4\n",
                        "Final Score: 4\n",
                        "\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -1\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -1\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -1\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -1\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -1\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -2\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -2\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -2\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -2\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -2\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -2\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -2\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -2\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -2\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -2\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -3\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -3\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -4\n",
                        "Agent at (1, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -5\n",
                        "Agent at (1, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty'}, Score: -5\n",
                        "Final Score: -5\n",
                        "\n",
                        "Agent at (3, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -1\n",
                        "Agent at (3, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -1\n",
                        "Agent at (2, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -2\n",
                        "Agent at (2, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -3\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -4\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -4\n",
                        "Agent at (3, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -5\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -6\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -6\n",
                        "Agent at (3, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -7\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -8\n",
                        "Agent at (2, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -9\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -10\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: 0\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: 0\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: 0\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: 0\n",
                        "Agent at (2, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -1\n",
                        "Agent at (2, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -2\n",
                        "Agent at (3, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Clean', (2, 0): 'Dirty', (2, 1): 'Dirty', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Dirty', (3, 3): 'Dirty'}, Score: -3\n",
                        "Final Score: -3\n",
                        "\n",
                        "Running State-Based Agent\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (1, 0): 'Clean', (1, 1): 'Dirty'}, Score: -1\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 9\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 8\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 18\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 17\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 27\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 26\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 25\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 24\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 23\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 22\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 21\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 20\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 19\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 18\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 17\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 16\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 15\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 14\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (1, 0): 'Clean', (1, 1): 'Clean'}, Score: 13\n",
                        "Final Score: 13\n",
                        "\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Dirty', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Dirty'}, Score: -1\n",
                        "Agent at (1, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Dirty', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Dirty'}, Score: -2\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Dirty', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Dirty'}, Score: -3\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Dirty', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 7\n",
                        "Agent at (2, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Dirty', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Dirty', (2, 2): 'Clean'}, Score: 6\n",
                        "Agent at (2, 1), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Dirty', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 16\n",
                        "Agent at (2, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Dirty', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 15\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Dirty', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 14\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 24\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 23\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 22\n",
                        "Agent at (2, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 21\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 20\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 19\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 18\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 17\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 16\n",
                        "Agent at (2, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 15\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 14\n",
                        "Agent at (0, 0), Environment: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Clean', (1, 1): 'Obstacle', (1, 2): 'Clean', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean'}, Score: 13\n",
                        "Final Score: 13\n",
                        "\n",
                        "Agent at (2, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 10\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Dirty', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 9\n",
                        "Agent at (2, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 19\n",
                        "Agent at (3, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 18\n",
                        "Agent at (3, 3), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 17\n",
                        "Agent at (3, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 16\n",
                        "Agent at (3, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Dirty', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 15\n",
                        "Agent at (3, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 25\n",
                        "Agent at (3, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Dirty', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 24\n",
                        "Agent at (3, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 34\n",
                        "Agent at (2, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Dirty', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 33\n",
                        "Agent at (2, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 43\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Dirty', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 42\n",
                        "Agent at (1, 0), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Clean', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 52\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Clean', (1, 1): 'Dirty', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 51\n",
                        "Agent at (1, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Clean', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 61\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Dirty', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Clean', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 60\n",
                        "Agent at (0, 1), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Clean', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 70\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Dirty', (0, 3): 'Dirty', (1, 0): 'Clean', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 69\n",
                        "Agent at (0, 2), Environment: {(0, 0): 'Dirty', (0, 1): 'Clean', (0, 2): 'Clean', (0, 3): 'Dirty', (1, 0): 'Clean', (1, 1): 'Clean', (1, 2): 'Obstacle', (1, 3): 'Dirty', (2, 0): 'Clean', (2, 1): 'Clean', (2, 2): 'Clean', (2, 3): 'Obstacle', (3, 0): 'Clean', (3, 1): 'Clean', (3, 2): 'Clean', (3, 3): 'Clean'}, Score: 79\n",
                        "Final Score: 79\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from agents import Agent, Environment, Thing\n",
                "import random\n",
                "\n",
                "# Define global tracking variables\n",
                "performance_score = 0\n",
                "visited_locations = set()\n",
                "\n",
                "def action_handler(environment, agent, action):\n",
                "    \"\"\"Handles agent actions and updates performance.\"\"\"\n",
                "    global performance_score\n",
                "    x, y = agent.location\n",
                "    \n",
                "    if action == 'MoveRight':\n",
                "        if (x + 1, y) in environment.locations and environment.status.get((x + 1, y)) != 'Obstacle':\n",
                "            agent.location = (x + 1, y)\n",
                "            agent.performance -= 1\n",
                "    elif action == 'MoveLeft':\n",
                "        if (x - 1, y) in environment.locations and environment.status.get((x - 1, y)) != 'Obstacle':\n",
                "            agent.location = (x - 1, y)\n",
                "            agent.performance -= 1\n",
                "    elif action == 'MoveUp':\n",
                "        if (x, y + 1) in environment.locations and environment.status.get((x, y + 1)) != 'Obstacle':\n",
                "            agent.location = (x, y + 1)\n",
                "            agent.performance -= 1\n",
                "    elif action == 'MoveDown':\n",
                "        if (x, y - 1) in environment.locations and environment.status.get((x, y - 1)) != 'Obstacle':\n",
                "            agent.location = (x, y - 1)\n",
                "            agent.performance -= 1\n",
                "    elif action == 'Clean':\n",
                "        if environment.status[agent.location] == 'Dirty':\n",
                "            agent.performance += 10\n",
                "        environment.status[agent.location] = 'Clean'\n",
                "    \n",
                "    performance_score = agent.performance\n",
                "\n",
                "class VacuumEnvironment(Environment):\n",
                "    \"\"\"Custom Vacuum Environment supporting a flexible grid.\"\"\"\n",
                "    def __init__(self, width=2, height=1, dirt_ratio=0.5, obstacles=None):\n",
                "        super().__init__()\n",
                "        self.locations = [(x, y) for x in range(width) for y in range(height)]\n",
                "        self.status = {loc: ('Dirty' if random.random() < dirt_ratio else 'Clean') for loc in self.locations}\n",
                "        if obstacles:\n",
                "            for obs in obstacles:\n",
                "                self.status[obs] = 'Obstacle'\n",
                "\n",
                "    def percept(self, agent):\n",
                "        \"\"\"Return location and status.\"\"\"\n",
                "        return agent.location, self.status.get(agent.location, 'Clean')\n",
                "\n",
                "    def execute_action(self, agent, action):\n",
                "        \"\"\"Execute agent action.\"\"\"\n",
                "        action_handler(self, agent, action)\n",
                "\n",
                "    def default_location(self, thing):\n",
                "        \"\"\"Return a default start location.\"\"\"\n",
                "        return random.choice([loc for loc in self.locations if self.status[loc] != 'Obstacle'])\n",
                "\n",
                "# Define Reflex Agent\n",
                "class ReflexVacuumAgent(Agent):\n",
                "    \"\"\"Simple Reflex Agent that cleans when dirty, moves otherwise.\"\"\"\n",
                "    def __init__(self):\n",
                "        def program(percept):\n",
                "            location, status = percept\n",
                "            visited_locations.add(location)\n",
                "            if status == 'Dirty':\n",
                "                return 'Clean'\n",
                "            return random.choice(['MoveLeft', 'MoveRight', 'MoveUp', 'MoveDown'])\n",
                "        super().__init__(program)\n",
                "\n",
                "# Define Randomized Agent\n",
                "class RandomVacuumAgent(Agent):\n",
                "    \"\"\"Agent that picks actions randomly.\"\"\"\n",
                "    def __init__(self):\n",
                "        def program(percept):\n",
                "            return random.choice(['MoveLeft', 'MoveRight', 'MoveUp', 'MoveDown', 'Clean'])\n",
                "        super().__init__(program)\n",
                "\n",
                "# Define State-Based Reflex Agent\n",
                "class StateReflexVacuumAgent(Agent):\n",
                "    \"\"\"State-based Reflex Agent that remembers visited locations and moves intelligently.\"\"\"\n",
                "    def __init__(self, env):\n",
                "        self.visited = set()\n",
                "        \n",
                "        def program(percept):\n",
                "            location, status = percept\n",
                "            self.visited.add(location)\n",
                "            if status == 'Dirty':\n",
                "                return 'Clean'\n",
                "            \n",
                "            # Find all possible moves\n",
                "            moves = [(location[0] + dx, location[1] + dy) for dx, dy in [(0,1), (0,-1), (1,0), (-1,0)]]\n",
                "            valid_moves = [move for move in moves if move in env.locations and env.status.get(move) != 'Obstacle']\n",
                "            \n",
                "            # Prioritize unvisited locations\n",
                "            unvisited = [move for move in valid_moves if move not in self.visited]\n",
                "            \n",
                "            if unvisited:\n",
                "                return move_towards(location, unvisited[0])\n",
                "            return move_towards(location, random.choice(valid_moves))\n",
                "        \n",
                "        super().__init__(program)\n",
                "\n",
                "def move_towards(current, target):\n",
                "    \"\"\"Helper function to determine the movement direction towards a target.\"\"\"\n",
                "    if target[0] > current[0]:\n",
                "        return 'MoveRight'\n",
                "    if target[0] < current[0]:\n",
                "        return 'MoveLeft'\n",
                "    if target[1] > current[1]:\n",
                "        return 'MoveUp'\n",
                "    return 'MoveDown'\n",
                "\n",
                "# Simulation Runner\n",
                "def run_simulation(agent_type, width=2, height=2, dirt_ratio=0.5, obstacles=None, steps=20):\n",
                "    \"\"\"Runs the vacuum world simulation.\"\"\"\n",
                "    global performance_score, visited_locations\n",
                "    performance_score = 0\n",
                "    visited_locations = set()\n",
                "    \n",
                "    environment = VacuumEnvironment(width, height, dirt_ratio, obstacles)\n",
                "    agent = agent_type(environment) if agent_type == StateReflexVacuumAgent else agent_type()\n",
                "    agent.location = environment.default_location(agent)\n",
                "    environment.add_thing(agent)\n",
                "    \n",
                "    for _ in range(steps):\n",
                "        environment.execute_action(agent, agent.program(environment.percept(agent)))\n",
                "        print(f\"Agent at {agent.location}, Environment: {environment.status}, Score: {performance_score}\")\n",
                "    \n",
                "    print(f\"Final Score: {performance_score}\\n\")\n",
                "\n",
                "# Run different agent types\n",
                "environments = [\n",
                "    (2, 2, 0.5, None),\n",
                "    (3, 3, 0.6, [(1, 1)]),\n",
                "    (4, 4, 0.7, [(1, 2), (2, 3)])\n",
                "]\n",
                "\n",
                "print(\"Running Reflex Agent\")\n",
                "for env in environments:\n",
                "    run_simulation(ReflexVacuumAgent, *env)\n",
                "\n",
                "print(\"Running Random Agent\")\n",
                "for env in environments:\n",
                "    run_simulation(RandomVacuumAgent, *env)\n",
                "\n",
                "print(\"Running State-Based Agent\")\n",
                "for env in environments:\n",
                "    run_simulation(StateReflexVacuumAgent, *env)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Analysis\n",
                "\n",
                "This experiment follows Exercise 2.11, where we implemented a vacuum-cleaner world simulator to evaluate agent performance in a customizable environment. The scoring system assigns **+10 points** for successfully cleaning a dirty location and **-1 point** for each movement action. The goal is to determine which agent type either Simple Reflex, Randomized Reflex, or State-Aware Reflex performs best under different environmental conditions as stated in the exercise on [1].\n",
                "\n",
                "For **Exercise 2.14**, we expanded the simulation to include an unknown environment and an unpredictable initial dirt configuration. Through this, we addressed the following key questions:\n",
                "\n",
                "### 1. Can a simple reflex agent be perfectly rational in this environment?\n",
                "A **Simple Reflex Agent** operates purely based on its current percept, meaning it lacks memory or a model of the environment [1]. As a result, it cannot achieve perfect rationality in this scenario. Since it only reacts to local conditions, it does not account for the larger structure of the environment or anticipate the presence of dirt beyond its immediate location. This limitation prevents it from making optimal long-term decisions, especially in more complex environments where efficient movement and dirt-tracking are required.\n",
                "\n",
                "### 2. Can a simple reflex agent with a randomized function outperform a simple reflex agent?\n",
                "Introducing randomized movement allows the agent to explore its environment more dynamically rather than getting stuck in repetitive loops. However, the results show that randomness does not consistently improve performance. While there are cases where random moves may accidentally lead to a more efficient cleaning sequence, in most situations, the Randomized Reflex Agent wastes time making unnecessary moves, reducing its overall efficiency. The unpredictable nature of its actions often leads to a lower performance score compared to the standard Simple Reflex Agent, which at least follows a set pattern.\n",
                "\n",
                "### 3. Can you design an environment where the randomized agent performs poorly?\n",
                "Yes, an environment that requires structured movement is particularly challenging for a randomized agent. For example:\n",
                "- A maze-like environment with obstacles, where a deterministic approach is needed to reach all locations.\n",
                "- A small grid where randomness can lead to excessive backtracking.\n",
                "- A highly structured world where the best action is always the same (e.g., always moving right or always cleaning first).\n",
                "\n",
                "In such environments, a random agent is likely to make inefficient movements, getting stuck in loops or failing to clean efficiently, as evidenced by its lower performance scores in our tests.\n",
                "\n",
                "### 4. Can a reflex agent with state outperform a simple reflex agent?\n",
                "Yes, the **State-Aware Reflex Agent** performs significantly better than both the **Simple Reflex Agent** and the **Randomized Reflex Agent**. The key reason for this improvement is its ability to remember visited locations, allowing it to:\n",
                "- **Avoid unnecessary movements** to already cleaned areas.\n",
                "- **Prioritize unvisited dirty locations** rather than moving blindly.\n",
                "- **Adapt better to unknown environments** by keeping track of explored regions.\n",
                "\n",
                "Our experiment confirms that state-awareness significantly enhances performance, as seen in the final scores where the State-Aware Reflex Agent consistently achieved higher values than the other two agents.\n",
                "\n",
                "### Final Insights\n",
                "This experiment demonstrates that an agent's performance is highly dependent on its ability to adapt and make informed decisions. While randomness can sometimes aid exploration, it is unreliable for efficiency. The State-Aware Reflex Agent emerges as the most effective, reinforcing the idea that incorporating memory and environmental awareness leads to superior decision-making in AI agents [1].\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conclusion\n",
                "\n",
                "The results highlight the critical role of state awareness in optimizing agent performance. The State-Aware Reflex Agent significantly outperforms both the Simple Reflex Agent and the Randomized Reflex Agent by leveraging memory to avoid redundant movements and prioritize uncleaned areas. \n",
                "\n",
                "This experiment reinforces fundamental Artificial Intelligence concepts, demonstrating how different agent architectures impact decision-making and efficiency in automated cleaning tasks. The findings emphasize that incorporating memory and structured decision-making leads to superior performance, especially in dynamic or unknown environments. These insights are valuable for designing intelligent autonomous agents capable of adapting to real-world challenges.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# References\n",
                "[1] Russell, S., & Norvig, P. (2020). *Artificial Intelligence: A Modern Approach* (4th ed.). Pearson.  \n",
                "[2] UC Berkeley Code Repository. (n.d.). Retrieved from [https://github.com/aimacode/aima-python](https://github.com/aimacode/aima-python).  "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
